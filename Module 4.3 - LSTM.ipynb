{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.3 - LSTM\n",
    "Link to original paper: http://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "\n",
    "Link to dataset generation: http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/reberGrammar.php\n",
    "\n",
    "## Lessons Learned\n",
    "The model in this project is a little different from all the other projects' models. In those projects, full vectorization can be achieve since there is alway full connectivity. In this project, full vectorization can be achieved too (which is what I did), but due to some sparse connectivities (the ones connect the gates with memory cells) and recurrent connectivities, it is a lot harder to implement and can be prone to bugs. \n",
    "\n",
    "One more note on the implementation of neural networks: deciding on where to store the previous activations and inputs should be a careful decision to make. I decided to put them in the lowest abstraction level possible, but it was not a good idea. Instead, I should have to put them \"as low in the abstraction level as possible\". The distinction between lowest level and as low as possible is the following: lowest level means to store activations and inputs in Sigmoid classes, Linear classes, and Softmax classes, hoping to simplify the forward/backward propgation steps; on the other hand, if you store those data as low as possible, you are storing those which have recurrent connections (e.g. gates) and those without (e.g. output) differently. \n",
    "\n",
    "In addtion to lessons I learned while implementing the LSTM, I learn some facts about this model (the one I implemented that is, which could be prone to bugs). Firstly, the model converges on greedy outputs. By greedy outputs, I mean the outputs that make statistical sense, but don't make temporal sense. In terms of statistics, it makes sense that the model abuses the frequency of the output neuron activates: the more a neuron activates, the more likely it is going to activate. However, in the Embedded Reber Grammar problem, statistics of a neuron's activation isn't everything. The model must sccount for the temporal dependency. It must look at the past time steps, which is the point of inventing LSTM in the first place. Secondly, neither Nadam nor SGD were able to converge toward using temporal dependency of the data. Oh, and in fact, SGD performed very simliar to Nadam if not slightly more stable.\n",
    "\n",
    "## Introduction\n",
    "LSTM was invented to solve the problem of vanishing gradients. Vanishing gradients is a problem occurs in RNNs, where the errors of RNN's weights exponentially decay as the RNN unfolds in time. As LSTM unfolds in time, however, the error stays constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/miniconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1120: UserWarning: Bad val \"$TEMPLATE_BACKEND\" on line #41\n",
      "\t\"backend      : $TEMPLATE_BACKEND\n",
      "\"\n",
      "\tin file \"/Users/macbook/.matplotlib/matplotlibrc\"\n",
      "\tKey backend: Unrecognized backend string \"$template_backend\": valid strings are [u'pgf', u'ps', u'Qt4Agg', u'GTK', u'GTKAgg', u'nbAgg', u'agg', u'cairo', u'MacOSX', u'GTKCairo', u'Qt5Agg', u'template', u'WXAgg', u'TkAgg', u'GTK3Cairo', u'GTK3Agg', u'svg', u'WebAgg', u'pdf', u'gdk', u'WX']\n",
      "  (val, error_details, msg))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "The dataset in this module is the embedded reber grammar problem.\n",
    "\n",
    "A sequence of chars will be generated based on the following graph, where each path is chosen at random. \n",
    "\n",
    "Each char is encoded as one hot vector. \n",
    "\n",
    "### The Embedded Reber Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Embedded Reber Grammar](http://christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/pics/embeddedReberGrammar.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = \"BTSXPVE \"\n",
    "\n",
    "graph = [[(1,5),('T','P')] , [(1,2),('S','X')], \\\n",
    "           [(3,5),('S','X')], [(6,),('E',)], \\\n",
    "           [(3,2),('V','P')], [(4,5),('V','T')], \\\n",
    "           [(6,6),(' ',' ')] ]\n",
    "\n",
    "def in_grammar(word):\n",
    "    if word[0] != 'B':\n",
    "        return False\n",
    "    node = 0    \n",
    "    for c in word[1:]:\n",
    "        transitions = graph[node]\n",
    "        try:\n",
    "            node = transitions[0][transitions[1].index(c)]\n",
    "        except ValueError: # using exceptions for flow control in python is common\n",
    "            return False\n",
    "    return True \n",
    "\n",
    "def generate_sequences(nb_samples, length):\n",
    "    assert nb_samples > 0\n",
    "    \n",
    "    in_sequences = []\n",
    "    out_sequences = []\n",
    "    \n",
    "    for i in range(nb_samples):\n",
    "        node = 0\n",
    "        in_sequences.append(\"B\")\n",
    "        out_sequences.append(\"\")\n",
    "        for _ in range(length):\n",
    "            transition = graph[node]\n",
    "            # randomly get a connection\n",
    "            j = np.random.randint(0,len(transition[0]))\n",
    "            # generate char sequence\n",
    "            in_sequences[i] += transition[1][j]\n",
    "            out_sequences[i] += transition[1][j]\n",
    "            node = transition[0][j]\n",
    "        assert in_grammar(in_sequences[i])\n",
    "    \n",
    "    assert len(in_sequences) == nb_samples\n",
    "    assert len(out_sequences) == nb_samples\n",
    "    \n",
    "    return in_sequences, out_sequences\n",
    "\n",
    "def char_to_one_hot(c):\n",
    "    assert chars.index(c) != -1\n",
    "    \n",
    "    one_hot = np.zeros(len(chars))\n",
    "    one_hot[chars.index(c)] = 1\n",
    "    \n",
    "    return list(one_hot)\n",
    "\n",
    "def one_hot_to_char(one_hot):\n",
    "    assert len(one_hot) == len(chars)\n",
    "    # must have one \"1\" encoded\n",
    "    assert one_hot.count(1) == 1\n",
    "    assert one_hot.count(0) == len(chars) - 1\n",
    "    assert one_hot.index(1) != -1\n",
    "    \n",
    "    return chars[one_hot.index(1)]\n",
    "    \n",
    "    \n",
    "def sequences_to_one_hots(sequences):\n",
    "    assert len(sequences) > 0\n",
    "    \n",
    "    one_hots = []\n",
    "    for s in sequences:\n",
    "        one_hots.append([])\n",
    "        for c in s:\n",
    "            one_hot = char_to_one_hot(c)\n",
    "            assert c == one_hot_to_char(one_hot)\n",
    "            one_hots[-1].append(one_hot)\n",
    "        assert len(one_hots[-1]) == len(s)\n",
    "        \n",
    "    assert len(sequences) == len(one_hots)\n",
    "    return one_hots\n",
    "\n",
    "def one_hots_to_sequences(one_hots):\n",
    "    assert len(one_hots) > 0\n",
    "    \n",
    "    sequences = []\n",
    "    for sample in one_hots:\n",
    "        sequences.append(\"\")\n",
    "        for one_hot in sample:\n",
    "            sequences[-1] += one_hot_to_char(one_hot)\n",
    "            assert char_to_one_hot(sequences[-1][-1]) == one_hot\n",
    "        assert len(sample) == len(sequences[-1])\n",
    "    \n",
    "    assert len(one_hots) == len(sequences)\n",
    "    return sequences\n",
    "\n",
    "def checkSymmetry(embedded = False):\n",
    "    if not embedded:\n",
    "        sequences, _ = generate_sequences(20, 20)\n",
    "    else:\n",
    "        sequences, _ = generate_embedded_sequence(20, 20)\n",
    "    one_hots = sequences_to_one_hots(sequences)\n",
    "    assert sequences == one_hots_to_sequences(one_hots)\n",
    "\n",
    "def generate_embedded_sequence(nb_samples, length):\n",
    "    in_sequences, out_sequences = generate_sequences(nb_samples, length)\n",
    "    for i in range(nb_samples):\n",
    "        embed = generate_sequences(1, length)[0][0].rstrip()\n",
    "        assert in_grammar(in_sequences[i])\n",
    "        in_sequences[i] = in_sequences[i][:2] + embed + in_sequences[i][2:]\n",
    "        in_sequences[i] = in_sequences[i][:length]\n",
    "        out_sequences[i] = out_sequences[i][:1] + embed + out_sequences[i][1:]\n",
    "        out_sequences[i] = out_sequences[i][:length - 1]\n",
    "        assert \"B\" + out_sequences[i] == in_sequences[i]\n",
    "        out_sequences[i] += \" \"\n",
    "    assert len(in_sequences) == nb_samples\n",
    "    return in_sequences, out_sequences\n",
    "\n",
    "checkSymmetry(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, t_train = generate_embedded_sequence(256, 20)\n",
    "X_test, t_test = generate_embedded_sequence(256, 20)\n",
    "\n",
    "X_train = sequences_to_one_hots(X_train)\n",
    "t_train = sequences_to_one_hots(t_train)\n",
    "X_test = sequences_to_one_hots(X_test)\n",
    "t_test = sequences_to_one_hots(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory\n",
    "Layers and gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.15\n",
    "momentum = 0.1\n",
    "itr = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split matrix top down (by columns)\n",
    "class MatrixSplitter(object):\n",
    "    def __init__(self, col_per_out):\n",
    "        assert isinstance(col_per_out, list)\n",
    "        assert len(col_per_out) > 1\n",
    "        \n",
    "        self.cols = [0]\n",
    "        col_per_out = col_per_out\n",
    "        for i in range(1, len(col_per_out)):\n",
    "            self.cols.append(self.cols[-1] + col_per_out[i - 1])\n",
    "        self.cols.remove(0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return np.split(X, self.cols, axis = -1)\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        return np.concatenate(Y, axis = -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(object):\n",
    "    def __init__(self):\n",
    "        self.activation = []\n",
    "    def forward(self, X):\n",
    "        self.activation.append(1 / (1 + np.exp(-X)))\n",
    "        return self.activation[-1]\n",
    "    def backward(self, outgrad):\n",
    "        activations = np.reshape(np.ravel(self.activation), (np.array(self.activation).shape[2],np.array(self.activation).shape[0] * np.array(self.activation).shape[1])).T\n",
    "        out = outgrad * activations * (1 - activations)\n",
    "        self.activation = []\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Softmax(object):\n",
    "    def forward(self, z):\n",
    "        self.activation = (np.exp(z) / np.sum(np.exp(z), axis=-1, keepdims=True))\n",
    "        return self.activation\n",
    "    def backward(self, outgrad):\n",
    "        out = (self.activation - outgrad) / self.activation.shape[0]\n",
    "        return out\n",
    "    def cost(self, T):\n",
    "        return - np.multiply(T, np.log(self.activation)).sum() / self.activation.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Nadam hyperparameters\n",
    "t = 0\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    def __init__(self, W, b = None):\n",
    "        self.W = W\n",
    "        if b == None:\n",
    "            self.b = 0\n",
    "        else:\n",
    "            self.b = b\n",
    "        self.X = []\n",
    "        \n",
    "        # for Nadam\n",
    "        self.param_grad = 0\n",
    "        self.b_grad = 0\n",
    "        self.param_v_t = 0\n",
    "        self.param_m_t = 0\n",
    "        self.b_m_t = 0\n",
    "        self.b_v_t = 0\n",
    "        \n",
    "        # SGD\n",
    "        self.param_update = 0\n",
    "        self.b_update = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.X.append(X)\n",
    "        return np.tensordot(X, self.W, axes=((-1), (0))) + self.b\n",
    "    \n",
    "    def backward(self, outgrad):\n",
    "        # calculate return value before we change weights\n",
    "        grad = np.tensordot(outgrad, self.W.T, axes=((-1), (0)))\n",
    "        # calculate weight changes\n",
    "        bpaxes = tuple(range(len(outgrad.shape) - 1))\n",
    "        if len(np.array(self.X).shape) == 4:\n",
    "            self.X = self.X[0]\n",
    "        if len(np.array(self.X).shape) - len(outgrad.shape) == 1:\n",
    "            self.X = np.reshape(self.X, (np.array(self.X).shape[0] * np.array(self.X).shape[1], np.array(self.X).shape[2]))\n",
    "        #print np.array(self.X).shape, outgrad.shape, bpaxes\n",
    "        param_grad = np.tensordot(np.array(self.X), outgrad, axes=(bpaxes, bpaxes))\n",
    "        b_grad = np.sum(outgrad, axis = bpaxes)\n",
    "        self.update_weights(param_grad, b_grad)\n",
    "        self.X = []\n",
    "        return grad\n",
    "    '''\n",
    "    #Nadam\n",
    "    def update_weights(self, param_grad, b_grad):\n",
    "        global learning_rate\n",
    "        global momentum\n",
    "        global t\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "        # update weights\n",
    "        self.param_m_t = beta_1 * self.param_m_t + (1 - beta_1) * self.param_grad\n",
    "        self.param_v_t = beta_2 * self.param_v_t + (1 - beta_2) * self.param_grad ** 2\n",
    "        g_corr_t = self.param_grad / (1 - beta_1 ** t)\n",
    "        m_corr_t = self.param_m_t / (1 - beta_1 ** (t + 1))\n",
    "        v_corr_t = self.param_v_t / (1 - beta_2 ** t)\n",
    "        m_nes_t = (1 - beta_1) * g_corr_t + beta_1 * m_corr_t\n",
    "        \n",
    "        self.W = self.W - learning_rate * m_nes_t / ((v_corr_t ** 0.5) + eps)\n",
    "        \n",
    "        # update bias\n",
    "        self.b_m_t = beta_1 * self.b_m_t + (1 - beta_1) * self.b_grad\n",
    "        self.b_v_t = beta_2 * self.b_v_t + (1 - beta_2) * self.b_grad ** 2\n",
    "        g_corr_t = self.b_grad / (1 - beta_1 ** t)\n",
    "        m_corr_t = self.b_m_t / (1 - beta_1 ** (t + 1))\n",
    "        v_corr_t = self.b_v_t / (1 - beta_2 ** t)\n",
    "        m_nes_t = (1 - beta_1) * g_corr_t + beta_1 * m_corr_t\n",
    "        \n",
    "        self.b = self.b - learning_rate * m_nes_t / ((v_corr_t ** 0.5) + eps)\n",
    "        \n",
    "        # save for later\n",
    "        self.param_grad = param_grad\n",
    "        self.b_grad = b_grad\n",
    "    '''\n",
    "    #stochastic gradient descent with momentum\n",
    "    def update_weights(self, param_grad, b_grad):\n",
    "        global learning_rate\n",
    "        global momentum\n",
    "        \n",
    "        param_momentum = self.param_update * momentum\n",
    "        b_momentum = self.b_update * momentum\n",
    "        \n",
    "        self.param_update = - param_grad * learning_rate + param_momentum\n",
    "        self.b_update = - b_grad * learning_rate + b_momentum\n",
    "        \n",
    "        #print \"grad: \" + repr(param_grad.shape), \"w: \" + repr(self.W.shape)\n",
    "        self.W += self.param_update \n",
    "        self.b += self.b_update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters - Nadam\n",
    "The following section contains the implementation of the gradient based optimization technique\n",
    "\n",
    "Pusedo-code: \n",
    "```python\n",
    "notations: \n",
    "    learning_rate\n",
    "    eps: fudge factor\n",
    "    beta_1, beta_2: decay rates\n",
    "    f(theta): objective function\n",
    "    g(theta): gradient function of f\n",
    "    theta_0: initial param\n",
    "    m_0 = 0: first moment\n",
    "    v_0 = 0: second raw moment\n",
    "    t_0 = 0: initial time step\n",
    "algorithm:\n",
    "    while not CONVERGE:\n",
    "        t += 1\n",
    "        g_t = g(theta_t-1)\n",
    "        m_t = beta_1 * m_t-1 + (1 - beta_1) * g_t\n",
    "        v_t = beta_2 * v_t-1 + (1 - beta_2) * g_t ** 2\n",
    "        g_corr_t = g_t / (1 - beta_1 ** t)\n",
    "        m_corr_t = m_t / (1 - beta_1 ** (t + 1))\n",
    "        v_corr_t = v_t / (1 - beta_2 ** t)\n",
    "        m_nes_t = (1 - beta_1) * g_corr_t + beta_1 * m_corr_t\n",
    "        theta_t = theta_t-1 - learning_rate * m_nes_t / (sqrt(v_corr_t) + eps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gate(object):\n",
    "    def __init__(self, W, b = None):\n",
    "        self.linear = Linear(W, b)\n",
    "        self.sigmoid = Sigmoid()\n",
    "    def forward(self, X):\n",
    "        return self.sigmoid.forward(self.linear.forward(X))\n",
    "    def backward(self, outgrad):\n",
    "        return self.linear.backward(self.sigmoid.backward(outgrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MemoryBlock(object):\n",
    "    def __init__(self, W_i, W_r, b, nb_of_memory_cells_per_block):\n",
    "        self.nb_of_inputs = W_i.shape[0]\n",
    "        self.nb_of_units = W_r.shape[1]\n",
    "        nb_of_memory_blocks = self.nb_of_units / (nb_of_memory_cells_per_block + 2)\n",
    "        nb_of_in_out_gates = nb_of_memory_blocks\n",
    "        self.nb_of_memory_cell_gates = nb_of_memory_blocks * nb_of_memory_cells_per_block\n",
    "        self.nb_of_memory_cells_per_block = nb_of_memory_cells_per_block\n",
    "        self.nb_of_gates = nb_of_in_out_gates * 2\n",
    "        \n",
    "        # let's call the g unit along with its incoming connections a \"memory cell gate\"\n",
    "        self.memory_cell_gate = Gate(np.concatenate([W_i[:, :self.nb_of_memory_cell_gates], W_r[:, :self.nb_of_memory_cell_gates]], axis = 0))\n",
    "        self.input_gate = Gate(np.concatenate([W_i[:, self.nb_of_memory_cell_gates:self.nb_of_memory_cell_gates+nb_of_in_out_gates], W_r[:, self.nb_of_memory_cell_gates:self.nb_of_memory_cell_gates+nb_of_in_out_gates]]), b[:, 0])\n",
    "        self.output_gate = Gate(np.concatenate([W_i[:, self.nb_of_memory_cell_gates+nb_of_in_out_gates:], W_r[:, self.nb_of_memory_cell_gates+nb_of_in_out_gates:]]), b[:, 1])\n",
    "        self.non_linearity = Sigmoid()\n",
    "        \n",
    "    def unfold(self, X):\n",
    "        self.h = []\n",
    "        self.g = []\n",
    "        self.y_in_his = []\n",
    "        self.y_out_his = []\n",
    "        \n",
    "        X_r = np.zeros((X.shape[0], self.nb_of_units))\n",
    "        S = np.zeros((X.shape[0], self.nb_of_memory_cell_gates))\n",
    "        output = np.zeros((X.shape[0], X.shape[1], self.nb_of_memory_cell_gates))\n",
    "                          \n",
    "        for k in range(X.shape[1]):\n",
    "            X_k = np.concatenate([X_r, X[:, k, :]], axis = -1)\n",
    "            self.g.append(self.memory_cell_gate.forward(X_k))\n",
    "            y_in = self.input_gate.forward(X_k)\n",
    "            y_out = self.output_gate.forward(X_k)\n",
    "            # update X_r partially\n",
    "            X_r[:, :self.nb_of_gates] = np.concatenate([y_in, y_out], axis = -1)\n",
    "            self.y_in_his.append(np.repeat(y_in, 2, axis = -1))\n",
    "            self.y_out_his.append(np.repeat(y_out, 2, axis = -1))\n",
    "            g_y_in = np.multiply(self.g[-1], self.y_in_his[-1])\n",
    "            S += g_y_in\n",
    "            self.h.append(self.non_linearity.forward(S))\n",
    "            h_y_out = np.multiply(self.h[-1], self.y_out_his[-1])\n",
    "            X_r[:,self.nb_of_gates:] =  h_y_out\n",
    "            output[:, k, :] = h_y_out\n",
    "            \n",
    "        self.h = np.array(self.h)\n",
    "        self.g = np.array(self.g)\n",
    "        self.y_in_his = np.array(self.y_in_his)\n",
    "        self.y_out_his = np.array(self.y_out_his)\n",
    "        \n",
    "        self.h = np.reshape(np.ravel(self.h.T), (self.h.shape[2],self.h.shape[0] * self.h.shape[1])).T\n",
    "        self.g = np.reshape(np.ravel(self.g.T), (self.g.shape[2],self.g.shape[0] * self.g.shape[1])).T\n",
    "        self.y_in_his = np.reshape(np.ravel(self.y_in_his.T), (self.y_in_his.shape[2],self.y_in_his.shape[0] * self.y_in_his.shape[1])).T\n",
    "        self.y_out_his = np.reshape(np.ravel(self.y_out_his.T), (self.y_out_his.shape[2],self.y_out_his.shape[0] * self.y_out_his.shape[1])).T\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def bptt(self, outgrad):\n",
    "        nb_of_time_steps = outgrad.shape[1]\n",
    "        nb_of_samples = outgrad.shape[0]\n",
    "        \n",
    "        outgrad = np.reshape(np.ravel(outgrad), (outgrad.shape[0] * outgrad.shape[1], outgrad.shape[2]))\n",
    "        g_y_out = np.multiply(self.h, outgrad)\n",
    "        g_h = np.multiply(self.h, self.y_out_his)\n",
    "        g_S = self.non_linearity.backward(g_h)\n",
    "        g_g_y_in = np.multiply(self.g, g_S)\n",
    "        g_g = np.multiply(self.g, self.y_in_his)\n",
    "        \n",
    "        # put everything back to shape\n",
    "        g_g_from_memory_cells = self.memory_cell_gate.backward(g_g)[:, :self.nb_of_inputs]\n",
    "        g_g_from_memory_cells = np.reshape(np.ravel(g_g_from_memory_cells), (nb_of_samples, nb_of_time_steps, g_g_from_memory_cells.shape[1]))\n",
    "        \n",
    "        #g_g_y_in = np.reshape(np.ravel(g_g), (nb_of_samples, nb_of_time_steps, g_g_y_in.shape[1]))\n",
    "        g_g_y_in = np.sum(np.reshape(np.ravel(g_g_y_in), (g_g_y_in.shape[0], g_g_y_in.shape[1] / 2, 2) ), axis = -1)\n",
    "        # only propgate the input errors\n",
    "        # because the errors from networks are not suppose to go back in time\n",
    "        g_g_from_input_gates = self.input_gate.backward(g_g_y_in)[:, :self.nb_of_inputs]\n",
    "        g_g_from_input_gates = np.reshape(np.ravel(g_g_from_input_gates), (nb_of_samples, nb_of_time_steps, g_g_from_input_gates.shape[1]))\n",
    "\n",
    "        #g_y_out = np.reshape(np.ravel(g_g), (nb_of_samples, nb_of_time_steps, g_y_out.shape[1]))\n",
    "        g_y_out = np.sum(np.reshape(np.ravel(g_y_out), (g_y_out.shape[0], g_y_out.shape[1] / 2, 2) ), axis = -1)\n",
    "        g_g_from_output_gates = self.output_gate.backward(g_y_out)[:, :self.nb_of_inputs]\n",
    "        g_g_from_output_gates = np.reshape(np.ravel(g_g_from_output_gates), (nb_of_samples, nb_of_time_steps, g_g_from_output_gates.shape[1]))\n",
    "        \n",
    "        return g_g_from_memory_cells + g_g_from_input_gates + g_g_from_output_gates\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM(object):\n",
    "    def __init__(self, n_in, n_out, nb_of_memory_blocks, nb_of_memory_cells_per_block):\n",
    "        \n",
    "        # input layer\n",
    "        n_hidden = nb_of_memory_blocks * nb_of_memory_cells_per_block\n",
    "        self.input = Linear(np.random.uniform(-1, 1, (n_in, n_in)))\n",
    "        \n",
    "        # connectivity\n",
    "        W_i = np.random.uniform(-1, 1, (n_in, n_hidden + 2 * nb_of_memory_blocks))\n",
    "        W_r = np.random.uniform(-1, 1, (n_hidden + 2 * nb_of_memory_blocks, n_hidden + 2 * nb_of_memory_blocks))\n",
    "        b = np.random.uniform(-1, 1, (nb_of_memory_blocks, 2))\n",
    "        \n",
    "        # hidden layer\n",
    "        self.hidden = MemoryBlock(W_i, W_r, b, nb_of_memory_cells_per_block)\n",
    "        \n",
    "        # output layer\n",
    "        self.output_linear = Linear(np.random.uniform(-1, 1, (n_hidden, n_out)))\n",
    "        self.output_non_linearity = Softmax()\n",
    "        \n",
    "        self.historical_costs = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #z_input = self.input.forward(X)\n",
    "        z_input = np.array(X)\n",
    "        z_hidden = self.hidden.unfold(z_input)\n",
    "        z_output_linear =  self.output_linear.forward(z_hidden)\n",
    "        return self.output_non_linearity.forward(z_output_linear)\n",
    "    \n",
    "    def backward(self, T):\n",
    "        outgrad = self.output_non_linearity.backward(T)\n",
    "        outgrad = self.output_linear.backward(outgrad)\n",
    "        outgrad = self.hidden.bptt(outgrad)\n",
    "        #outgrad = self.input.backward(outgrad)\n",
    "        self.historical_costs.append(self.output_non_linearity.cost(T))\n",
    "        return outgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/miniconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "lstm = LSTM(8, 8, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:46<00:00,  4.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(itr)):\n",
    "    X_train, t_train = generate_embedded_sequence(256, 20)\n",
    "\n",
    "    X_train = sequences_to_one_hots(X_train)\n",
    "    t_train = sequences_to_one_hots(t_train)\n",
    "\n",
    "    lstm.forward(X_train)\n",
    "    lstm.backward(t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWIiISE4KFiIikpOChYiI5PT/AcpoZabuEQh8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1081069d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lstm.historical_costs, 'b-')\n",
    "plt.xlabel('itr')\n",
    "plt.ylabel('cost')\n",
    "plt.title('LSTM Cost over time')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]]\n",
      "['T']\n",
      "[' ']\n"
     ]
    }
   ],
   "source": [
    "X_test, t_test = generate_embedded_sequence(1, 1)\n",
    "X_test = sequences_to_one_hots(X_test)\n",
    "t_test = sequences_to_one_hots(t_test)\n",
    "\n",
    "idx = np.argmax(lstm.forward(X_test), axis = 2)[0]\n",
    "out = []\n",
    "for i in idx:\n",
    "    out.append([0,0,0,0,0,0,0,0])\n",
    "    out[-1][i] = 1\n",
    "print one_hots_to_sequences([out])\n",
    "print one_hots_to_sequences(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BTBTSXXVPX']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_embedded_sequence(1, 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.319092434522659, 30.221080922352868, 30.643779199659278, 30.686330581806828, 31.045844879419608, 30.427760527818808, 30.212239454344918, 30.16245575380033, 30.819688839932223, 30.712528673221264]\n"
     ]
    }
   ],
   "source": [
    "print lstm.historical_costs[len(lstm.historical_costs)-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0. -2. -0. -1. -0. -3.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(np.sum(lstm.output_linear.W, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -0.  1. -1. -1. -2. -2. -0. -0. -1. -1.  1.  1. -0. -1. -1.  0. -2.\n",
      " -0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(np.sum(lstm.hidden.output_gate.linear.W, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -5. -16. -12. -14.  -8.  -9.  -9.  -7.  -6.  -7.  -7.  -7.  -4.  -3.  -1.\n",
      "  -2.  -2.  -3.  -2.  -5.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(np.sum(lstm.hidden.input_gate.linear.W, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -52. -254. -217. -213. -128. -120. -148. -146. -115. -113. -108. -107.\n",
      "  -35.  -44.  -34.  -33.  -31.  -48.  -33.  -92.]\n"
     ]
    }
   ],
   "source": [
    "print np.round(np.sum(lstm.hidden.memory_cell_gate.linear.W, axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.argmax(lstm.forward([[[0,0,0,0,0,1,0,0]]]), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
